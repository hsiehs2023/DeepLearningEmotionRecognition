{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlJ8anRP7b3c",
        "outputId": "25a44bd0-87a5-49d4-d68c-26ca07256bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcuJx-3A73_i",
        "outputId": "22f4ee98-0f94-4f41-ce63-36f43ff7aec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/cs4262/EmotionDetection\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/cs4262/EmotionDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-CVVxI47__d"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t76_dZGF8G48"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/cs4262/EmotionDetection/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSA9QBZc8MPR"
      },
      "outputs": [],
      "source": [
        "ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qBcqICm9dpf"
      },
      "outputs": [],
      "source": [
        "!chmod +x getDataset.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iorZ2rQ-CJl"
      },
      "outputs": [],
      "source": [
        "! bash getDataset.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxK8bMoUgZf1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load training data from CSV\n",
        "train_df = pd.read_csv(\"/content/gdrive/MyDrive/cs4262/EmotionDetection/datasets/train_data.csv\")\n",
        "\n",
        "# Load testing data from CSV\n",
        "test_df = pd.read_csv(\"/content/gdrive/MyDrive/cs4262/EmotionDetection/datasets/test_data.csv\")\n",
        "\n",
        "# Load images and labels from training data\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "    image = load_img(row['trainSet'], target_size=(32, 32))\n",
        "    image = img_to_array(image) / 255.0  # Normalize pixel values\n",
        "    train_images.append(image)\n",
        "    train_labels.append(row['trainLabel'])\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "\n",
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "train_labels = to_categorical(train_labels_encoded)  # One-hot encode labels\n",
        "\n",
        "# Load images and labels from testing data\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    image = load_img(row['testSet'], target_size=(32, 32))\n",
        "    image = img_to_array(image) / 255.0  # Normalize pixel values\n",
        "    test_images.append(image)\n",
        "    test_labels.append(row['testabel'])\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "# Label encoding for test labels\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "test_labels = to_categorical(test_labels_encoded)  # One-hot encode labels\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc7z5yn5DMIU"
      },
      "outputs": [],
      "source": [
        "# Define the CNN architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Train the model with data augmentation and early stopping\n",
        "history = model.fit(datagen.flow(train_images, train_labels, batch_size=64, shuffle=True),\n",
        "                    steps_per_epoch=len(train_images) // 64,\n",
        "                    epochs=100,\n",
        "                    validation_data=(val_images, val_labels),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the accuracy curves\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions = model.predict(test_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(np.argmax(test_labels, axis=1), predicted_labels)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Compute precision\n",
        "precision = precision_score(np.argmax(test_labels, axis=1), predicted_labels, average='weighted')\n",
        "print(f'Precision: {precision}')\n",
        "\n",
        "# Compute recall\n",
        "recall = recall_score(np.argmax(test_labels, axis=1), predicted_labels, average='weighted')\n",
        "print(f'Recall: {recall}')\n",
        "\n",
        "# Compute F1 score\n",
        "f1 = f1_score(np.argmax(test_labels, axis=1), predicted_labels, average='weighted')\n",
        "print(f'F1 score: {f1}')"
      ],
      "metadata": {
        "id": "g0p_p5A0QXcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnK3Q8f9qAWo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Generate a random index to select a random image from the test dataset\n",
        "image_index = random.randint(0, len(test_images) - 1)\n",
        "\n",
        "# Selected the random image and its corresponding label\n",
        "selected_image = test_images[image_index]\n",
        "selected_label = np.argmax(test_labels[image_index])\n",
        "\n",
        "# Display the random image\n",
        "plt.imshow(selected_image)\n",
        "plt.axis('off')\n",
        "plt.title(f'Actual Label: {selected_label}')\n",
        "plt.show()\n",
        "\n",
        "# Make a prediction on the selected image\n",
        "prediction = model.predict(np.expand_dims(selected_image, axis=0))\n",
        "predicted_label = np.argmax(prediction)\n",
        "confidence = np.max(prediction)\n",
        "\n",
        "# Mapping from label indices to class names\n",
        "class_names = ['0 - Angry', '1 - Disgust', '2 - Fear', '3 - Happy', '4 - Neutral',\n",
        "               '5 - Sad', '6 - Surprise']\n",
        "\n",
        "# Display the model's prediction\n",
        "print(f\"Predicted Label: {class_names[predicted_label]}, Confidence: {confidence}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}